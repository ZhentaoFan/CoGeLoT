# @package _global_

defaults:
  - override /trainer: gpu.yaml

# DMOG using A40s.

# As we need to maintain the batch size of 128, but on a single GPU, there's a bunch of tricks we
# need to do to make this work.
datamodule:
  # TODO: What is the largest batch size we can fit?
  # Choices: 1, 2, 4, 8, 16, 32, 64, 128
  batch_size: 32
  num_workers: 10

trainer:
  accumulate_grad_batches: 4
  devices: 1
  num_nodes: 1
