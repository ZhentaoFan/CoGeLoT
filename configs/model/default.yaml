defaults:
  - _self_
  - action_encoder@policy.action_encoder: vima
  - action_decoder@policy.action_decoder: vima
  - transformer_decoder@policy.transformer_decoder: their_vima

_target_: cogelot.models.VIMALightningModule

should_shuffle_obj_per_observations: false

policy:
  _target_: cogelot.modules.policy.Policy
  embed_dim: 768

  transformer_decoder: ???
  action_encoder: ???
  action_decoder: ???

  pose_action_tokenizer:
    _target_: cogelot.modules.tokenizers.PoseActionTokenizer

  obj_encoder:
    _target_: vima.nn.ObjEncoder
    transformer_emb_dim: ${..embed_dim}
    views: ["front", "top"]
    vit_output_dim: ${.transformer_emb_dim}
    vit_resolution: 32
    vit_patch_size: 16
    vit_width: ${.transformer_emb_dim}
    vit_layers: 4
    vit_heads: 24
    bbox_mlp_hidden_dim: ${.transformer_emb_dim}
    bbox_mlp_hidden_depth: 2

  end_effector_encoder:
    _target_: vima.nn.Embedding
    num_embeddings: 2
    embedding_dim: 2

  obs_fusion_layer:
    _target_: torch.nn.Linear
    in_features:
      _target_: builtins.sum
      _convert_: all
      _args_:
        - - ${.....obj_encoder.transformer_emb_dim}
          - ${.....end_effector_encoder.embedding_dim}
    out_features: ${..embed_dim}

  prompt_embedding:
    _target_: cogelot.modules.text_encoders.T5TextEmbedder
    pretrained_model: t5-base

  prompt_encoder:
    _target_: cogelot.modules.text_encoders.T5PromptEncoder.from_pretrained
    pretrained_model_name_or_path: t5-base

  prompt_obj_post_layer:
    _target_: vima.nn.build_mlp
    input_dim: ${..embed_dim}
    hidden_dim: ${.input_dim}
    output_dim: ${.hidden_dim}
    hidden_depth: 2
