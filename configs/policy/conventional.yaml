defaults:
  - default.yaml
  - _self_

# Conventional Transformer Decoder, but with 'special flavours'.
#
# Special flavours (just like in VIMA):
# - FFN has bias disabled
# - Use GEGLU instead of ReLU in FFN
# - Use PreNorm instead of PostNorm
#
# Links:
# [1]: https://github.com/lucidrains/x-transformers
# [2]: https://wandb.ai/dalle-mini/dalle-mini/reports/An-Evaluation-of-Transformer-Variants--VmlldzoxNjk4MTIw

transformer_decoder:
  _target_: cogelot.nn.decoders.XTransformerDecoder
  max_seq_len: 512
  use_abs_pos_emb: True

  attn_layers:
    _target_: x_transformers.Decoder

    dim: ${..dim_in}
    depth: 11
    heads: 24

    causal: true
    cross_attend: true

    pre_norm: true

    layer_dropout: 0.1
    ff_dropout: ${.layer_dropout}
    attn_dropout: ${.layer_dropout}
    cross_attn_tokens_dropout: ${.layer_dropout}

    # Use GEGLU instead of ReLU in FFN
    ff_glu: true

    # Disable bias in FFN
    ff_no_bias: true
    # Inner dimension of FFN = 4 * 768 = 3072
    ff_mult: 4

    # Use flash attention (default in pytorch 2.0+)
    attn_flash: true
