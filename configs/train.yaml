# @package _global_

defaults:
  - _self_
  - trainer: cpu.yaml
    # The chosen policy will automatically be set to the model.policy
  - policy@model.policy: vima.yaml
  # If you want to debug, then just run like `python train.py debug=default`
  - debug: null

  # Set defaults for Hydra's logging and outputs
  - hydra: default.yaml

task_name: "train"

output_dir: ${hydra:runtime.output_dir}

seed: 1000

datamodule:
  _target_: cogelot.data.vima_datamodule.VIMADataModule
  hf_datasets_repo_name: amitkparekh/vima
  batch_size: 16
  num_workers: 64
  dataloader_kwargs:
    pin_memory: false
    prefetch_factor: 8
    # https://forums.fast.ai/t/last-training-batch-is-dropped/39321/2
    drop_last: true

model:
  _target_: cogelot.models.vima.VIMALightningModule

  optimizer_partial_fn:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 0.0001
    weight_decay: 0

  lr_scheduler_partial_fn:
    _target_: cogelot.training.lr_scheduler.get_cosine_schedule_with_warmup_and_lr_end
    _partial_: true
    num_warmup_steps: 7000
    num_training_steps: 24000
    # https://github.com/vimalabs/VIMA/issues/16#issuecomment-1623627926
    lr_end: 1e-7
