_target_: pytorch_lightning.trainer.Trainer

default_root_dir: ${output_dir}

gradient_clip_val: 1.0

min_epochs: 1 # prevents early stopping

# https://github.com/vimalabs/VIMA/issues/16#issuecomment-1622973970
# max_epochs: 10
# Although they ran for 10 epochs, experiments have shown that nothing interesting happens after 6
# epochs, because the LR rate has plummeted to 1e-7 by then.
max_epochs: 6

accelerator: cpu
devices: 1

# mixed precision for extra speed-up
# precision: 16

# perform a validation loop every N training epochs
check_val_every_n_epoch: 1

# set True to to ensure deterministic results
# makes training slower but gives more reproducibility than just setting seeds
deterministic: False

logger:
  - _target_: pytorch_lightning.loggers.WandbLogger
    save_dir: ${output_dir}
    project: CoGeLoT
    entity: pyop
    log_model: False
  - _target_: cogelot.common.checkpoint.HuggingFaceModelLogger
    repo_id: amitkparekh/cogelot
    use_hf_transfer: false

callbacks:
  - _target_: pytorch_lightning.callbacks.RichModelSummary
    max_depth: -1
  - _target_: pytorch_lightning.callbacks.TQDMProgressBar
  - _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: step

  - _target_: pytorch_lightning.callbacks.ModelCheckpoint
    dirpath: ${output_dir}/checkpoints
    save_top_k: 1
    monitor: val_loss
    mode: min
    filename: "{epoch:02d}-{val_loss:.2f}"
