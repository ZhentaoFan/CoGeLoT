# Conventional Transformer Decoder layer with cross-attention, but with 'special flavours'.
#
# Special flavours (just like in VIMA):
# - FFN has bias disabled
# - Use GEGLU instead of ReLU in FFN
# - Use PreNorm instead of PostNorm
#
# Links:
# [1]: https://github.com/lucidrains/x-transformers
# [2]: https://wandb.ai/dalle-mini/dalle-mini/reports/An-Evaluation-of-Transformer-Variants--VmlldzoxNjk4MTIw

attn_layers:
  _target_: x_transformers.Decoder

  cross_attend: true

  dim: ${model.policy.embed_dim}
  depth: 11
  heads: 24

  pre_norm: true

  layer_dropout: 0.1
  ff_dropout: ${.layer_dropout}
  attn_dropout: ${.layer_dropout}
  cross_attn_tokens_dropout: ${.layer_dropout}

  # Use GEGLU instead of ReLU in FFN
  ff_glu: true

  # Disable bias in FFN
  ff_no_bias: true
  # Inner dimension of FFN = 4 * 768 = 3072
  ff_mult: 4

  # Use flash attention (default in pytorch 2.0+)
  attn_flash: true
